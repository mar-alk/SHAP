{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation, visualization, modeling, and evaluation\n",
    "\n",
    "# Basic Libraries\n",
    "import numpy as np  # For numerical operations on arrays and matrices\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt  # For creating static plots and figures\n",
    "import seaborn as sns  # For enhanced data visualization, particularly for statistical plots\n",
    "\n",
    "# Data Preprocessing and Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler  # To scale feature values to a specified range, often [0, 1]\n",
    "\n",
    "# Machine Learning Models and Tools\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest Regressor model for predictive tasks\n",
    "import catboost  # CatBoost library for handling categorical data efficiently in machine learning\n",
    "from catboost import CatBoostRegressor  # Gradient-boosting model specific to CatBoost for regression tasks\n",
    "import shap  # SHAP (SHapley Additive exPlanations) for interpretability and feature importance\n",
    "\n",
    "# Hyperparameter Tuning and Model Evaluation\n",
    "from sklearn.model_selection import GridSearchCV  # Grid Search for hyperparameter tuning across multiple models\n",
    "from sklearn.metrics import r2_score  # R-squared metric to evaluate the performance of regression models\n",
    "\n",
    "# Deep Learning Framework (TensorFlow and Keras for Neural Networks)\n",
    "import tensorflow as tf  # TensorFlow for deep learning framework\n",
    "from tensorflow import keras  # High-level neural network API for TensorFlow\n",
    "from tensorflow.keras import models, layers, optimizers  # Core modules for creating and training neural networks\n",
    "\n",
    "# Import necessary classes and functions from Keras for building and training a neural network model\n",
    "from keras.models import Sequential  # Sequential API for stacking neural network layers linearly\n",
    "from keras.layers import Dense, Dropout  # Dense is a fully connected layer; Dropout helps prevent overfitting\n",
    "from keras.callbacks import EarlyStopping  # Callback function to stop training early if validation performance stops improving# Define the Artificial Neural Network (ANN) model architecture with regularization and dropout\n",
    "\n",
    "# Initialize a Sequential model for a linear stack of layers\n",
    "ANN = Sequential()\n",
    "\n",
    "# Add the first Dense (fully connected) layer with:\n",
    "# - 128 units (neurons), ReLU activation function, L2 regularization to reduce overfitting, and input dimension of 1024\n",
    "ANN.add(Dense(128, input_dim=1024, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add a Dropout layer with 50% dropout rate to reduce overfitting by setting half of the units to zero randomly\n",
    "ANN.add(Dropout(0.5))\n",
    "\n",
    "# Add a second Dense layer with 64 units, ReLU activation, and L2 regularization\n",
    "ANN.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add another Dropout layer with 50% dropout rate\n",
    "ANN.add(Dropout(0.5))\n",
    "\n",
    "# Add the output Dense layer with a single unit and sigmoid activation for binary classification\n",
    "# Sigmoid activation function is used to output probabilities for binary classification\n",
    "ANN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary crossentropy loss, Adam optimizer, and accuracy as the evaluation metric\n",
    "# - binary_crossentropy is suitable for binary classification tasks\n",
    "# - Adam optimizer is efficient and often provides good convergence\n",
    "ANN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Add EarlyStopping callback to prevent overfitting\n",
    "# - Monitors the validation loss and stops training if it does not improve for 5 consecutive epochs (patience=5)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "\n",
    "# Save the model summary to a text file for record-keeping\n",
    "with open('ANN_Classification.txt', 'w') as f:\n",
    "    ANN.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Print the model summary to verify the architecture\n",
    "print(ANN.summary())\n",
    "\n",
    "# Train (fit) the ANN model on the training data with validation data and early stopping\n",
    "# - Epochs are set to 100 (maximum), though early stopping may end training sooner\n",
    "# - batch_size=32 defines the number of samples processed before the model’s internal parameters are updated\n",
    "# - Validation data (X_test, Y_test) is used to monitor performance and apply early stopping\n",
    "history2 = ANN.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test, Y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance on the test data\n",
    "# - evaluate() returns the loss and accuracy on the test data, providing a measure of model generalization\n",
    "loss, accuracy = ANN.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)  # Display the test accuracy\n",
    "\n",
    "from keras.regularizers import l2  # L2 regularization adds a penalty to the model to reduce overfitting by penalizing large weights\n",
    "\n",
    "# Train-Test Split for Model Validation\n",
    "from sklearn.model_selection import train_test_split  # To split the dataset into training and testing sets\n",
    "\n",
    "# Oversampling for Imbalanced Data Handling\n",
    "from imblearn.over_sampling import SMOTE  # SMOTE (Synthetic Minority Oversampling Technique) to address data imbalance\n",
    "\n",
    "# Classification Model from CatBoost\n",
    "from catboost import CatBoostClassifier  # CatBoost Classifier for handling categorical and imbalanced data in classification tasks\n",
    "\n",
    "# Metrics for Model Evaluation\n",
    "from sklearn import metrics  # General metrics for model evaluation (e.g., accuracy, precision, recall)\n",
    "from sklearn.metrics import confusion_matrix  # Confusion matrix to evaluate classification model performance\n",
    "\n",
    "# Visualization of Model Architecture\n",
    "import graphviz  # For rendering graphical representations of decision trees\n",
    "\n",
    "# Keras Deep Learning Models and Layers (used with Sequential API)\n",
    "from keras.models import Sequential  # Sequential model type for stackable layers\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout  # Layers for CNN architecture (Conv1D, MaxPooling1D, etc.)\n",
    "from keras.callbacks import EarlyStopping  # Early stopping to prevent overfitting during training\n",
    "from keras.regularizers import l2  # Regularization technique to avoid overfitting by adding penalty terms to the model\n",
    "from keras.utils import plot_model  # Utility for plotting the model architecture visually\n",
    "\n",
    "# Machine Learning Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier model for classification tasks\n",
    "from sklearn import svm  # Support Vector Machine (SVM) classifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # Gradient Boosting Regressor model for regression tasks\n",
    "\n",
    "# Ensure unique imports for optimal performance\n",
    "from sklearn.model_selection import GridSearchCV  # Already imported above but retained for clarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea96b9f",
   "metadata": {},
   "source": [
    "# Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89771ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ECFP4 data into a DataFrame\n",
    "# The dataset is assumed to contain molecular descriptors (ECFP6) and biological activity values (e.g., pXC50).\n",
    "# Replace `data_CHEMBL203-ECFP6.csv` with the actual file path if necessary.\n",
    "Data_06 = pd.read_csv(r\"data_CHEMBL203-ECFP6.csv\")  # Load CSV data into a DataFrame named Data_06\n",
    "\n",
    "# Classification Task: Define a binary class based on pXC50 threshold\n",
    "# The threshold for binary classification is set to pXC50 > 6:\n",
    "# If pXC50 > 6, the class label is set to 1 (active); otherwise, it is set to 0 (inactive).\n",
    "Class = Data_06['pXC50'] >= 6  # Create a boolean Series indicating if pXC50 values are >= 6\n",
    "\n",
    "# Initialize an empty list to store binary class labels\n",
    "Classes = []\n",
    "for i in Class:\n",
    "    # Assign class 1 for active compounds (pXC50 >= 6) and class 0 for inactive compounds (pXC50 < 6)\n",
    "    if i == True:\n",
    "        Classes.append(1)\n",
    "    elif i == False:\n",
    "        Classes.append(0)\n",
    "\n",
    "# Add the binary class labels as a new column 'Class' in the DataFrame\n",
    "Data_06['Class'] = Classes  # Insert binary labels for classification task\n",
    "\n",
    "# Define feature matrix (X06) and target variable (Y06)\n",
    "# Exclude 'Smiles' (molecular structure representation), 'pXC50' (activity value), and 'Class' (target variable) from features\n",
    "X06 = Data_06.drop(['Smiles', 'pXC50', 'Class'], axis=1)  # Feature matrix for model input\n",
    "Y06 = Data_06['Class'].values.reshape(-1, 1)  # Target variable reshaped as a column vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba64a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for handling data imbalance\n",
    "# SMOTE (Synthetic Minority Oversampling Technique) is used to oversample the minority class, making the dataset more balanced.\n",
    "# This can help improve model performance, especially in classification tasks with imbalanced data.\n",
    "\n",
    "# Initialize SMOTE with a 'minority' sampling strategy, which oversamples the minority class to balance it with the majority class.\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)  # 'random_state=42' ensures reproducibility\n",
    "\n",
    "# Apply SMOTE on feature matrix (X) and target variable (Y) to create a balanced dataset\n",
    "X_sm, y_sm = smote.fit_resample(X, Y)  # X_sm and y_sm contain the resampled features and target labels\n",
    "\n",
    "# Verify class distribution across train, validation, and test sets after SMOTE\n",
    "# Convert the target values from train, validation, and test sets into a list for checking distribution\n",
    "y_s = list(Y_train) + list(Y_val) + list(Y_test)  # Aggregate all target values into a single list for verification\n",
    "\n",
    "# Calculate and display the count of each class label in the resampled data to verify balance\n",
    "pd.Series(y_s).value_counts()  # Count occurrences of each class label (0 or 1) after SMOTE resampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137494ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "# First, we create a training set and a temporary test set from the resampled dataset\n",
    "# Then, the test set is further split to create separate validation and final test sets.\n",
    "\n",
    "# Step 1: Split the data into 80% training and 20% test sets\n",
    "# 'test_size=0.2' specifies that 20% of the data will be reserved for testing, and 80% for training.\n",
    "X_train, X_tes, Y_train, Y_tes = train_test_split(X_sm, y_sm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the temporary test set (X_tes and Y_tes) into validation and test sets\n",
    "# We use a 50% split here, which divides the initial 20% test set into 10% validation and 10% final test sets.\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_tes, Y_tes, test_size=0.5, random_state=42)\n",
    "\n",
    "# Result:\n",
    "# - X_train, Y_train: Training set (80% of the data)\n",
    "# - X_val, Y_val: Validation set (10% of the data)\n",
    "# - X_test, Y_test: Testing set (10% of the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train a CatBoost classification model using training and validation sets\n",
    "# The CatBoost model is an optimized gradient boosting algorithm particularly effective for categorical data\n",
    "\n",
    "# Step 1: Set model parameters for the CatBoostClassifier\n",
    "params = {\n",
    "    'iterations': 3000,  # Total number of boosting iterations\n",
    "    'learning_rate': 0.01,  # Learning rate to control the size of each step during gradient descent\n",
    "    'depth': 7,  # Depth of each tree, controlling model complexity\n",
    "    'eval_metric': 'Accuracy',  # Evaluation metric to optimize (here we focus on classification accuracy)\n",
    "    'verbose': 200,  # Output training logs every 200 iterations for monitoring\n",
    "    'od_type': \"Iter\",  # Overfitting detector type, here set to stop after a fixed number of bad iterations\n",
    "    'od_wait': 1000,  # Overfitting detector: number of iterations to wait after the last improvement before stopping\n",
    "    'random_seed': 2  # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Step 2: Initialize the CatBoost classifier with the specified parameters\n",
    "cat_model = CatBoostClassifier(**params)  # Instantiate the classifier with defined hyperparameters\n",
    "\n",
    "# Step 3: Train the model on the training set and validate on the validation set\n",
    "# - eval_set specifies the validation data for evaluation during training\n",
    "# - use_best_model=False disables stopping at the best iteration (useful if exploring model behavior manually)\n",
    "# - plot=True provides a graphical plot of the model’s performance on training and validation sets over iterations\n",
    "cat_model.fit(\n",
    "    X_train, Y_train,  # Training data (features and labels)\n",
    "    eval_set=(X_val, Y_val),  # Validation data (features and labels)\n",
    "    use_best_model=False,  # Disable early stopping at best iteration\n",
    "    plot=True  # Plot training and validation accuracy over iterations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47056f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained CatBoost model on the test set\n",
    "# The classification report provides metrics such as precision, recall, and F1-score for each class.\n",
    "\n",
    "# Print classification report to summarize model performance\n",
    "# metrics.classification_report() compares true labels (Y_test) with model predictions (cat_model.predict(X_test))\n",
    "# and outputs detailed evaluation metrics:\n",
    "# - Precision: The accuracy of positive predictions\n",
    "# - Recall: The ability of the model to capture positive instances\n",
    "# - F1-score: The harmonic mean of precision and recall, useful for imbalanced data\n",
    "# - Support: The number of occurrences of each class in Y_test\n",
    "\n",
    "print(metrics.classification_report(Y_test, cat_model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f057e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix to visually assess the model's performance in classification\n",
    "# The confusion matrix shows the counts of true positives, true negatives, false positives, and false negatives\n",
    "\n",
    "# Step 1: Set up the plot dimensions and style\n",
    "plt.figure(figsize=(10, 5))  # Define figure size for the plot\n",
    "plt.xticks(size=17, weight='bold')  # Customize x-axis tick label font size and weight\n",
    "plt.yticks(size=17, weight='bold')  # Customize y-axis tick label font size and weight\n",
    "plt.title('Confusion Matrix for Catboost Algorithm', size=20, weight='bold')  # Set title with font size and weight\n",
    "\n",
    "# Step 2: Generate the confusion matrix and plot it as a heatmap\n",
    "# - confusion_matrix() computes the matrix from true labels (Y_test) and model predictions\n",
    "# - sns.heatmap() creates a heatmap to represent the matrix values visually\n",
    "sns.heatmap(\n",
    "    confusion_matrix(Y_test, cat_model.predict(X_test)),  # Confusion matrix data for heatmap\n",
    "    annot=True,  # Annotate each cell with the count values\n",
    "    fmt='g',  # Display counts as integers (without scientific notation)\n",
    "    annot_kws={\"size\": 20, 'weight': 'bold'}  # Customize annotation font size and weight\n",
    ")\n",
    "\n",
    "# Step 3: Save the confusion matrix plot to a file\n",
    "# - dpi=3# SHAP (SHapley Additive exPlanations) Analysis for Feature Importance in CatBoost Model\n",
    "# SHAP helps interpret the model by showing feature importance and how individual features contribute to predictions.\n",
    "\n",
    "# Step 1: Prepare data for SHAP analysis\n",
    "all_preds = cat_model.predict(X_test)  # Generate predictions for the test set to verify model's behavior\n",
    "X_df = pd.DataFrame(X_test)  # Convert X_test into a DataFrame for easier manipulation\n",
    "x_df = X_df.copy(deep=True)  # Create a deep copy of the test feature set for SHAP analysis\n",
    "\n",
    "# Create an additional DataFrame copy to append predictions for further analysis if needed\n",
    "x_df_1st = x_df.copy(deep=True)\n",
    "x_df_1st['1st'] = all_preds  # Add a column with model predictions\n",
    "\n",
    "# Reset index of the DataFrames for easier access to feature data\n",
    "x_df = x_df.reset_index().drop('index', axis=1)\n",
    "x_df_1st = x_df_1st.reset_index().drop('index', axis=1)\n",
    "\n",
    "# Step 2: Apply SHAP analysis using TreeExplainer for CatBoost model\n",
    "# TreeExplainer provides SHAP values (impact scores) for each feature on each prediction\n",
    "shap_values = shap.TreeExplainer(cat_model).shap_values(x_df)\n",
    "\n",
    "# Step 3: Generate SHAP summary plot to visualize feature importance\n",
    "# shap.summary_plot() shows the average effect of each feature on model output\n",
    "# - plot_size and max_display control plot size and the number of features displayed\n",
    "# - plot_type='dot' displays feature importance with dots (one per instance per feature)\n",
    "shap.summary_plot(shap_values, x_df, plot_size=(10, 10), show=False, plot_type='dot', max_display=10)\n",
    "plt.title('SHAP for CatBoost', weight='bold', size=20)  # Add title with custom font settings\n",
    "plt.xticks(size=20, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=20, weight='bold')  # Customize y-axis tick labels\n",
    "plt.savefig('SHAP 06 Classify', dpi=300, bbox_inches='tight')  # Save plot as 'SHAP 06 Classify.png'\n",
    "\n",
    "# Step 4: Calculate average absolute SHAP values to assess feature importance\n",
    "# We compute the mean absolute SHAP values across all instances to get an importance score for each feature\n",
    "feature_imp = np.mean(np.abs(shap_values), axis=0)\n",
    "feature_imp.shape  # Check the shape to ensure compatibility with features\n",
    "\n",
    "# Step 5: Identify the top 10 most important features based on SHAP values\n",
    "ind = feature_imp.argsort()[-10:]  # Get indices of the top 10 features by importance\n",
    "ind = ind[::-1]  # Arrange indices in descending order of importance\n",
    "\n",
    "# Display the top 10 features and their SHAP importance values\n",
    "np.array(x_df.columns)[ind]  # Names of the top 10 important features\n",
    "feature_imp[ind]  # Corresponding importance scores for the top features\n",
    "\n",
    "# Step 6: Plot top 10 features by SHAP importance as a bar chart\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "plot = sns.barplot(x=np.array(x_df.columns)[ind], y=feature_imp[ind], color=[0.1, 0.2, 0.1], order=ind)  # Bar plot\n",
    "plot.set_xticklabels(plot.get_xticklabels(), horizontalalignment='center', size=12)  # Adjust x-axis labels\n",
    "plt.yticks(size=15, weight='bold')  # Customize y-axis tick labels\n",
    "plt.xticks(size=20, rotation=20, weight='bold')  # Customize x-axis tick labels with rotation\n",
    "plt.ylabel('Shap feature absolute importance', size=15, weight='bold')  # Set y-axis label\n",
    "plt.savefig('SHAP Feature Importance CB 06', dpi=100, bbox_inches='tight')  # Save plot as 'SHAP Feature Importance CB 06.png'\n",
    "00 sets the resolution for the saved image\n",
    "# - bbox_inches='tight' removes extra whitespace around the plot\n",
    "plt.savefig('Catboost CM 06', dpi=300, bbox_inches='tight')  # Save plot as 'Catboost CM 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional Neural Network (CNN) model architecture with regularization and dropout\n",
    "# Regularization and dropout layers are added to reduce overfitting and improve model generalization\n",
    "\n",
    "# Initialize a Sequential model\n",
    "CNN = Sequential()\n",
    "\n",
    "# Add the first Conv1D layer with 64 filters, kernel size 3, ReLU activation, and L2 regularization\n",
    "# - input_shape=(1024, 1) indicates the input feature shape, assuming 1024 features and 1 channel per instance\n",
    "CNN.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(1024, 1), kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add a second Conv1D layer with 64 filters, kernel size 3, ReLU activation, and L2 regularization\n",
    "CNN.add(Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add MaxPooling layer to down-sample the output by reducing the spatial dimensions\n",
    "CNN.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the 3D output to a 1D vector for input into fully connected (Dense) layers\n",
    "CNN.add(Flatten())\n",
    "\n",
    "# Add a fully connected (Dense) layer with 64 units, ReLU activation, and L2 regularization\n",
    "CNN.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add a Dropout layer with a 50% dropout rate to prevent overfitting by randomly setting half of the units to zero\n",
    "CNN.add(Dropout(0.5))\n",
    "\n",
    "# Add the output Dense layer with a single unit and sigmoid activation for binary classification\n",
    "CNN.add(Dense(1, activation='sigmoid'))  # Output layer uses sigmoid for probability output in binary classification\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "# - binary_crossentropy is suitable for binary classification tasks\n",
    "# - Adam optimizer is efficient and often provides quick convergence\n",
    "CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define EarlyStopping callback to prevent overfitting\n",
    "# - Monitors the validation loss and stops training if it does not improve for 5 consecutive epochs (patience=5)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "\n",
    "# Save the model summary to a text file for record-keeping\n",
    "with open('CNN_Classification.txt', 'w') as f:\n",
    "    CNN.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Print the model summary to verify the architecture\n",
    "print(CNN.summary())\n",
    "\n",
    "# Save a graphical representation of the CNN model architecture\n",
    "keras.utils.plot_model(CNN, \"my_first_model.png\", dpi=300)  # Save as 'my_first_model.png' with 300 DPI for clarity\n",
    "\n",
    "# Fit the model to the training data with validation data and early stopping\n",
    "# - Epochs set to 100 as the maximum, but early stopping may stop training earlier\n",
    "# - Validation data (X_val, Y_val) is used to monitor performance and apply early stopping\n",
    "history = CNN.fit(X_train, Y_train, epochs=100, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 1: Training and validation loss over epochs\n",
    "plt.figure(figsize=(8, 5))  # Define figure size for the plot\n",
    "\n",
    "# Plot the training and validation loss using data stored in the history object\n",
    "# - history.history['loss'] contains training loss values over epochs\n",
    "# - history.history['val_loss'] contains validation loss values over epochs\n",
    "plt.plot(history.history['loss'], linewidth=5, linestyle='-')  # Training loss plot with custom line width and style\n",
    "plt.plot(history.history['val_loss'], linewidth=5, linestyle='-')  # Validation loss plot with same style\n",
    "\n",
    "# Add titles and labels with custom font sizes and weights\n",
    "plt.title('CNN Model Loss', size=20, weight='bold')\n",
    "plt.xlabel('Epoch', size=20, weight='bold')\n",
    "plt.ylabel('Loss', size=20, weight='bold')\n",
    "plt.xticks(size=15, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=15, weight='bold')  # Customize y-axis tick labels\n",
    "\n",
    "# Add a legend to differentiate training and validation loss\n",
    "plt.legend(['Train', 'Validation'], loc='upper right', prop={\"size\": 15, 'weight': 'bold'})\n",
    "\n",
    "# Save the plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('CNN Loss 06', dpi=100, bbox_inches='tight')  # Save as 'CNN Loss 06.png'\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Plot 2: Training and validation accuracy over epochs\n",
    "plt.figure(figsize=(8, 5))  # Define figure size for the plot\n",
    "\n",
    "# Plot the training and validation accuracy using data stored in the history object\n",
    "# - history.history['accuracy'] contains training accuracy over epochs\n",
    "# - history.history['val_accuracy'] contains validation accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], linewidth=5, linestyle='-')  # Training accuracy plot\n",
    "plt.plot(history.history['val_accuracy'], linewidth=5, linestyle='-')  # Validation accuracy plot\n",
    "\n",
    "# Add titles and labels with custom font sizes and weights\n",
    "plt.title('CNN Model Accuracy', size=20, weight='bold')\n",
    "plt.xlabel('Epoch', size=20, weight='bold')\n",
    "plt.ylabel('Accuracy', size=20, weight='bold')\n",
    "plt.xticks(size=15, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=15, weight='bold')  # Customize y-axis tick labels\n",
    "\n",
    "# Add a legend to differentiate training and validation accuracy\n",
    "plt.legend(['Train', 'Validation'], loc='lower right', prop={\"size\": 15, 'weight': 'bold'})\n",
    "\n",
    "# Save the plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('CNN Accuracy 06', dpi=300, bbox_inches='tight')  # Save as 'CNN Accuracy 06.png'\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a412c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Evaluation on Test Data\n",
    "# This section generates predictions on the test set, converts probabilities to binary class predictions,\n",
    "# and prints a classification report to evaluate the model's performance.\n",
    "\n",
    "# Generate predictions on the test set\n",
    "# - CNN.predict(X_test) outputs probability scores (values between 0 and 1) for each instance in the test set\n",
    "preds = CNN.predict(X_test)\n",
    "\n",
    "# Convert probability scores to binary predictions\n",
    "# - Threshold is set at 0.5: predictions >= 0.5 are classified as 1 (positive class), and predictions < 0.5 as 0 (negative class)\n",
    "y_pred_binary = (preds >= 0.5).astype(int)\n",
    "\n",
    "# Print the classification report for model evaluation\n",
    "# - metrics.classification_report() compares true labels (Y_test) with predicted labels (y_pred_binary)\n",
    "# - Outputs precision, recall, F1-score, and support for each class (0 and 1 in binary classification)\n",
    "print(metrics.classification_report(Y_test, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae780710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for CNN model predictions\n",
    "# The confusion matrix provides insights into the number of correct and incorrect predictions for each class (0 and 1).\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Define figure size for the plot\n",
    "\n",
    "# Customize tick label font size and weight for readability\n",
    "plt.xticks(size=17, weight='bold')\n",
    "plt.yticks(size=17, weight='bold')\n",
    "\n",
    "# Set the title of the plot with custom font size and weight\n",
    "plt.title('Confusion Matrix for CNN', size=20, weight='bold')\n",
    "\n",
    "# Generate and plot the confusion matrix as a heatmap\n",
    "# - confusion_matrix(Y_test, y_pred_binary) creates a matrix comparing true vs. predicted labels\n",
    "# - annot=True displays count values in each cell\n",
    "# - fmt='g' ensures values are displayed as integers (no scientific notation)\n",
    "# - cmap='Blues' uses a blue color map to visually differentiate matrix values\n",
    "# - annot_kws adjusts the size and weight of annotation text for clarity\n",
    "sns.heatmap(confusion_matrix(Y_test, y_pred_binary), annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 20, 'weight': 'bold'})\n",
    "\n",
    "# Save the plot to a file with specified resolution and bounding box\n",
    "plt.savefig('CNN CM 06', dpi=100, bbox_inches='tight')  # Save as 'CNN CM 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Artificial Neural Network (ANN) model architecture with regularization and dropout\n",
    "\n",
    "# Initialize a Sequential model for a linear stack of layers\n",
    "ANN = Sequential()\n",
    "\n",
    "# Add the first Dense (fully connected) layer with:\n",
    "# - 128 units (neurons), ReLU activation function, L2 regularization to reduce overfitting, and input dimension of 1024\n",
    "ANN.add(Dense(128, input_dim=1024, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add a Dropout layer with 50% dropout rate to reduce overfitting by setting half of the units to zero randomly\n",
    "ANN.add(Dropout(0.5))\n",
    "\n",
    "# Add a second Dense layer with 64 units, ReLU activation, and L2 regularization\n",
    "ANN.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Add another Dropout layer with 50% dropout rate\n",
    "ANN.add(Dropout(0.5))\n",
    "\n",
    "# Add the output Dense layer with a single unit and sigmoid activation for binary classification\n",
    "# Sigmoid activation function is used to output probabilities for binary classification\n",
    "ANN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary crossentropy loss, Adam optimizer, and accuracy as the evaluation metric\n",
    "# - binary_crossentropy is suitable for binary classification tasks\n",
    "# - Adam optimizer is efficient and often provides good convergence\n",
    "ANN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Add EarlyStopping callback to prevent overfitting\n",
    "# - Monitors the validation loss and stops training if it does not improve for 5 consecutive epochs (patience=5)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "\n",
    "# Save the model summary to a text file for record-keeping\n",
    "with open('ANN_Classification.txt', 'w') as f:\n",
    "    ANN.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Print the model summary to verify the architecture\n",
    "print(ANN.summary())\n",
    "\n",
    "# Train (fit) the ANN model on the training data with validation data and early stopping\n",
    "# - Epochs are set to 100 (maximum), though early stopping may end training sooner\n",
    "# - batch_size=32 defines the number of samples processed before the model’s internal parameters are updated\n",
    "# - Validation data (X_test, Y_test) is used to monitor performance and apply early stopping\n",
    "history2 = ANN.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test, Y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance on the test data\n",
    "# - evaluate() returns the loss and accuracy on the test data, providing a measure of model generalization\n",
    "loss, accuracy = ANN.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Accuracy:\", accuracy)  # Display the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 1: Training and validation loss over epochs for the ANN model\n",
    "plt.figure(figsize=(8, 5))  # Define figure size for the plot\n",
    "\n",
    "# Plot the training and validation loss values recorded in the training history\n",
    "# - history2.history['loss'] contains training loss over epochs\n",
    "# - history2.history['val_loss'] contains validation loss over epochs\n",
    "plt.plot(history2.history['loss'], linewidth=5, linestyle='-')  # Training loss plot with custom line width and style\n",
    "plt.plot(history2.history['val_loss'], linewidth=5, linestyle='-')  # Validation loss plot with same style\n",
    "\n",
    "# Set plot title and axis labels with font size and weight\n",
    "plt.title('ANN Model Loss', size=20, weight='bold')\n",
    "plt.xlabel('Epoch', size=20, weight='bold')\n",
    "plt.ylabel('Loss', size=20, weight='bold')\n",
    "plt.xticks(size=15, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=15, weight='bold')  # Customize y-axis tick labels\n",
    "\n",
    "# Add a legend to differentiate training and validation loss\n",
    "plt.legend(['Train', 'Validation'], loc='upper right', prop={\"size\": 15, 'weight': 'bold'})\n",
    "\n",
    "# Save the plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('ANN Loss 06', dpi=100, bbox_inches='tight')  # Save as 'ANN Loss 06.png'\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Plot 2: Training and validation accuracy over epochs for the ANN model\n",
    "plt.figure(figsize=(8, 5))  # Define figure size for the plot\n",
    "\n",
    "# Plot the training and validation accuracy values recorded in the training history\n",
    "# - history2.history['accuracy'] contains training accuracy over epochs\n",
    "# - history2.history['val_accuracy'] contains validation accuracy over epochs\n",
    "plt.plot(history2.history['accuracy'], linewidth=5, linestyle='-')  # Training accuracy plot\n",
    "plt.plot(history2.history['val_accuracy'], linewidth=5, linestyle='-')  # Validation accuracy plot\n",
    "\n",
    "# Set plot title and axis labels with font size and weight\n",
    "plt.title('ANN Model Accuracy', size=20, weight='bold')\n",
    "plt.xlabel('Epoch', size=20, weight='bold')\n",
    "plt.ylabel('Accuracy', size=20, weight='bold')\n",
    "plt.xticks(size=15, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=15, weight='bold')  # Customize y-axis tick labels\n",
    "\n",
    "# Add a legend to differentiate training and validation accuracy\n",
    "plt.legend(['Train', 'Validation'], loc='lower right', prop={\"size\": 15, 'weight': 'bold'})\n",
    "\n",
    "# Save the plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('ANN Accuracy 06', dpi=100, bbox_inches='tight')  # Save as 'ANN Accuracy 06.png'\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the ANN model on the test set\n",
    "# This section generates predictions, converts them to binary classifications, and prints a classification report\n",
    "# to assess the model's performance, followed by a confusion matrix plot.\n",
    "\n",
    "# Step 1: Generate predictions on the test set\n",
    "# - ANN.predict(X_test) outputs probability scores (between 0 and 1) for each instance in the test set\n",
    "preds = ANN.predict(X_test)\n",
    "\n",
    "# Step 2: Convert probabilities to binary predictions\n",
    "# - Threshold of 0.5 is used: predictions >= 0.5 are classified as 1 (positive class), predictions < 0.5 as 0 (negative class)\n",
    "y_pred_binary = (preds >= 0.5).astype(int)\n",
    "\n",
    "# Step 3: Print the classification report for performance metrics\n",
    "# - metrics.classification_report() compares true labels (Y_test) with predicted labels (y_pred_binary)\n",
    "# - Outputs precision, recall, F1-score, and support for each class (0 and 1 in binary classification)\n",
    "print(metrics.classification_report(Y_test, y_pred_binary))\n",
    "\n",
    "# Step 4: Plot confusion matrix for the ANN model\n",
    "# The confusion matrix visually displays the counts of true positives, false positives, true negatives, and false negatives\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Set figure size for the plot\n",
    "\n",
    "# Customize tick labels and title for readability\n",
    "plt.xticks(size=17, weight='bold')\n",
    "plt.yticks(size=17, weight='bold')\n",
    "plt.title('Confusion Matrix for ANN', size=20, weight='bold')\n",
    "\n",
    "# Generate and plot the confusion matrix as a heatmap\n",
    "# - confusion_matrix(Y_test, y_pred_binary) computes the confusion matrix from true vs. predicted labels\n",
    "# - annot=True displays the count values in each cell\n",
    "# - fmt='g' formats values as integers (no scientific notation)\n",
    "# - cmap='Blues' applies a blue color map to differentiate values visually\n",
    "# - annot_kws adjusts font size and weight of annotations for better readability\n",
    "sns.heatmap(confusion_matrix(Y_test, y_pred_binary), annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 20, 'weight': 'bold'})\n",
    "\n",
    "# Save the confusion matrix plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('ANN CM 06', dpi=100, bbox_inches='tight')  # Save as 'ANN CM 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for Random Forest model predictions\n",
    "# This confusion matrix visualizes the number of correct and incorrect predictions for each class (0 and 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Set figure size for the plot\n",
    "\n",
    "# Customize x and y-axis tick labels for better readability\n",
    "plt.xticks(size=17, weight='bold')\n",
    "plt.yticks(size=17, weight='bold')\n",
    "\n",
    "# Set the plot title with custom font size and weight\n",
    "plt.title('Confusion Matrix for Random Forest', size=20, weight='bold')\n",
    "\n",
    "# Generate and plot the confusion matrix as a heatmap\n",
    "# - confusion_matrix(Y_test, RF.predict(X_test)) calculates the confusion matrix from true vs. predicted labels\n",
    "# - annot=True displays count values in each cell\n",
    "# - fmt='g' formats values as integers (disables scientific notation)\n",
    "# - cmap='Blues' applies a blue color gradient to differentiate matrix values\n",
    "# - annot_kws adjusts font size and weight for annotation text in the heatmap cells\n",
    "sns.heatmap(confusion_matrix(Y_test, RF.predict(X_test)), annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 20, 'weight': 'bold'})\n",
    "\n",
    "# Save the confusion matrix plot to a file with specified resolution and tight bounding box to minimize extra space\n",
    "plt.savefig('RF CM 06', dpi=100, bbox_inches='tight')  # Save as 'RF CM 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dced20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels into X_train and Y_train, and test data into X_test\n",
    "# Ensure that your training and test datasets are loaded in the variables before running this code\n",
    "\n",
    "# Step 1: Define the SVM model with a radial basis function (RBF) kernel and balanced class weights\n",
    "# - kernel='rbf' specifies the use of a radial basis function kernel, which is suitable for nonlinear data\n",
    "# - class_weight='balanced' adjusts weights inversely proportional to class frequencies to handle imbalance\n",
    "# - probability=True enables probability estimates, which can be useful for further model evaluation\n",
    "SVC = svm.SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "\n",
    "# Step 2: Fit the SVM model to the training data\n",
    "# - X_train and Y_train are used as the input features and target labels for model training\n",
    "SVC.fit(X_train, Y_train)\n",
    "\n",
    "# Step 3: Predict on the test data\n",
    "# - SVC.predict(X_test) generates predictions for each instance in the test set\n",
    "y_pred = SVC.predict(X_test)\n",
    "\n",
    "# Step 4: Display the classification report for model evaluation\n",
    "# - metrics.classification_report() compares true labels (Y_test) with predicted labels (y_pred)\n",
    "# - Outputs precision, recall, F1-score, and support for each class, providing a detailed view of model performance\n",
    "print(metrics.classification_report(Y_test, y_pred))\n",
    "\n",
    "# Step 5 (Optional): Plot confusion matrix for the SVM model\n",
    "plt.figure(figsize=(10, 5))  # Set figure size for the plot\n",
    "plt.xticks(size=17, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=17, weight='bold')  # Customize y-axis tick labels\n",
    "plt.title('Confusion Matrix for SVM', size=20, weight='bold')  # Set plot title with styling\n",
    "\n",
    "# Generate and plot the confusion matrix as a heatmap\n",
    "sns.heatmap(confusion_matrix(Y_test, y_pred), annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 20, 'weight': 'bold'})\n",
    "\n",
    "# Save the confusion matrix plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('SVM CM 06', dpi=100, bbox_inches='tight')  # Save as 'SVM CM 06.png'\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb136c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for SVC model predictions\n",
    "# The confusion matrix provides insights into the number of correct and incorrect predictions for each class (0 and 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Set figure size for better visualization\n",
    "\n",
    "# Customize the x and y-axis tick labels for improved readability\n",
    "plt.xticks(size=17, weight='bold')\n",
    "plt.yticks(size=17, weight='bold')\n",
    "\n",
    "# Set plot title with custom font size and weight\n",
    "plt.title('Confusion Matrix for SVC', size=20, weight='bold')\n",
    "\n",
    "# Generate and plot the confusion matrix as a heatmap\n",
    "# - confusion_matrix(Y_test, SVC.predict(X_test)) calculates the confusion matrix from true vs. predicted labels\n",
    "# - annot=True displays count values in each cell\n",
    "# - fmt='g' formats values as integers to avoid scientific notation\n",
    "# - cmap='Blues' applies a blue color map for visual clarity\n",
    "# - annot_kws adjusts font size and weight of annotation text in each cell\n",
    "sns.heatmap(confusion_matrix(Y_test, SVC.predict(X_test)), annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 20, 'weight': 'bold'})\n",
    "\n",
    "# Save the confusion matrix plot as an image file with specified resolution and tight bounding box\n",
    "plt.savefig('SVC CM 06', dpi=100, bbox_inches='tight')  # Save as 'SVC CM 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5177d",
   "metadata": {},
   "source": [
    "# Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data for the regression task\n",
    "\n",
    "# Step 1: Prepare features (X_Pos) and target variable (Y_Pos) for positive class samples\n",
    "# - 'Smiles', 'Class', and 'pXC50' columns are removed from X_Pos as they are not used as features for prediction\n",
    "X_Pos = Data_Positive.drop(['Smiles', 'Class', 'pXC50'], axis=1)\n",
    "\n",
    "# Extract 'pXC50' column as the target variable and reshape it into a 2D array\n",
    "Y_Pos = Data_Positive['pXC50'].values.reshape(-1, 1)\n",
    "\n",
    "# Convert Y_Pos into a pandas Series for compatibility with various regression functions\n",
    "Y_Pos = pd.Series(Y_Pos.reshape(1, -1)[0])\n",
    "\n",
    "# Step 2: Rename columns in X_Pos to integer column headers\n",
    "# - This simplifies column naming for easier access in regression models\n",
    "new_clm_heads = [x for x in range(len(X_Pos.columns))]  # Create a list of integers based on the number of columns\n",
    "X_Pos.columns = new_clm_heads  # Rename columns in X_Pos\n",
    "\n",
    "# Filter Data_Positive for samples with pXC50 >= 6\n",
    "# - Data_Positive now contains only those samples where 'pXC50' is greater than or equal to 6\n",
    "Data_Positive = Data_06[Data_06['pXC50'] >= 6]\n",
    "\n",
    "# Display Data_Positive to verify filtered data\n",
    "Data_Positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with active molecules into training, validation, and testing sets\n",
    "# Stratified splits are used to maintain the distribution of the target variable in each subset\n",
    "\n",
    "# Step 1: Initial split of data into 90% training and 10% temporary test set\n",
    "# - test_size=0.1 reserves 10% of the data as a temporary test set\n",
    "# - stratify=df.iloc[:, 1] ensures the split is stratified based on the target variable's distribution\n",
    "X_train, X_tes, Y_train, Y_tes = train_test_split(X_Pos, df, test_size=0.1, random_state=42, stratify=df.iloc[:, 1])\n",
    "\n",
    "# Step 2: Further split the temporary test set (X_tes and Y_tes) into validation and final test sets\n",
    "# - test_size=0.5 reserves half of the temporary test set as validation and the other half as the final test set\n",
    "# - stratify=Y_tes.iloc[:, 1] maintains the target distribution within each subset\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_tes, Y_tes, test_size=0.5, random_state=42, stratify=Y_tes.iloc[:, 1])\n",
    "\n",
    "# Results:\n",
    "# - X_train, Y_train: Training set (90% of the data)\n",
    "# - X_val, Y_val: Validation set (5% of the data)\n",
    "# - X_test, Y_test: Testing set (5% of the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train a CatBoost regression model with specified parameters\n",
    "# CatBoost is a gradient boosting model that handles categorical data efficiently\n",
    "\n",
    "# Step 1: Set the model parameters for the CatBoostRegressor\n",
    "params = {\n",
    "    'iterations': 5000,         # Number of boosting iterations\n",
    "    'learning_rate': 0.01,      # Learning rate for controlling the step size during gradient descent\n",
    "    'depth': 8,                 # Depth of each tree, which affects model complexity\n",
    "    'eval_metric': 'R2',        # Evaluation metric set to R-squared for regression\n",
    "    'verbose': 200,             # Output training progress every 200 iterations\n",
    "    'od_type': \"Iter\",          # Overfitting detector type; stops training if no improvement within a given number of iterations\n",
    "    'od_wait': 1000,            # Number of iterations to wait after the last improvement before stopping\n",
    "    'random_seed': 8            # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Step 2: Initialize the CatBoostRegressor model with the defined parameters\n",
    "cat_model = CatBoostRegressor(**params)\n",
    "\n",
    "# Step 3: Train the model on the training data, with validation data for monitoring\n",
    "# - eval_set=(X_val, Y_val.iloc[:,0]) provides the validation set for model evaluation during training\n",
    "# - use_best_model=True stops training after the iteration with the best validation score\n",
    "# - plot=True displays a graphical representation of training and validation scores over iterations\n",
    "cat_model.fit(\n",
    "    X_train, Y_train.iloc[:, 0],   \n",
    "    eval_set=(X_val, Y_val.iloc[:, 0]), \n",
    "    use_best_model=True, \n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Step 4: Model Evaluation on training, validation, and test sets\n",
    "# - Generate predictions for each dataset using the trained model\n",
    "pred_train = cat_model.predict(X_train)\n",
    "pred_val = cat_model.predict(X_val)\n",
    "pred_test = cat_model.predict(X_test)\n",
    "\n",
    "# Calculate and print RMSE (Root Mean Squared Error) for each dataset to evaluate model accuracy\n",
    "# RMSE is calculated as the square root of the mean squared error between actual and predicted values\n",
    "print(np.sqrt(np.mean((Y_train.iloc[:, 0] - pred_train) ** 2)))  # Training RMSE\n",
    "print(np.sqrt(np.mean((Y_val.iloc[:, 0] - pred_val) ** 2)))      # Validation RMSE\n",
    "print(np.sqrt(np.mean((Y_test.iloc[:, 0] - pred_test) ** 2)))    # Test RMSE\n",
    "\n",
    "# Calculate and print R-squared (R²) scores for each dataset to evaluate model fit\n",
    "# R² score indicates the proportion of variance in the target explained by the model; higher is better\n",
    "print('\\n')\n",
    "print(r2_score(Y_train.iloc[:, 0], pred_train))  # Training R² score\n",
    "print(r2_score(Y_val.iloc[:, 0], pred_val))      # Validation R² score\n",
    "print(r2_score(Y_test.iloc[:, 0], pred_test))    # Test R² score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc40cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SHAP analysis to the CatBoost model for feature importance and interpretability\n",
    "# SHAP (SHapley Additive exPlanations) helps interpret the model by showing how each feature contributes to predictions\n",
    "\n",
    "# Step 1: Generate predictions on the test set\n",
    "# - all_preds contains the predicted values for each instance in the test set\n",
    "all_preds = cat_model.predict(X_test)\n",
    "\n",
    "# Step 2: Prepare data for SHAP analysis\n",
    "# Convert X_test to a DataFrame for SHAP compatibility and easier manipulation\n",
    "X_df = pd.DataFrame(X_test)\n",
    "\n",
    "# Create deep copies of the DataFrame to avoid affecting the original data\n",
    "x_df = X_df.copy(deep=True)\n",
    "x_df_1st = x_df.copy(deep=True)\n",
    "\n",
    "# Add predictions as a new column in x_df_1st for reference or further analysis if needed\n",
    "x_df_1st['1st'] = all_preds\n",
    "\n",
    "# Reset indices in x_df and x_df_1st for a clean structure\n",
    "x_df = x_df.reset_index().drop('index', axis=1)\n",
    "x_df_1st = x_df_1st.reset_index().drop('index', axis=1)\n",
    "\n",
    "# Step 3: Generate SHAP values using TreeExplainer for the CatBoost model\n",
    "# TreeExplainer calculates SHAP values, showing each feature's contribution to predictions\n",
    "shap_values = shap.TreeExplainer(cat_model).shap_values(x_df)\n",
    "\n",
    "# Step 4: Display SHAP summary plot for feature importance\n",
    "# The summary plot shows the impact of each feature on the model's output\n",
    "# - plot_size specifies dimensions, max_display limits the number of displayed features to top 10, and plot_type is set to 'dot'\n",
    "shap.summary_plot(shap_values, x_df, plot_size=(10, 10), show=False, plot_type='dot', max_display=10)\n",
    "plt.title('SHAP for CatBoost', weight='bold', size=20)  # Add title with custom font settings\n",
    "plt.xticks(size=20, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=20, weight='bold')  # Customize y-axis tick labels\n",
    "plt.savefig('Reg SHAP 06', dpi=100, bbox_inches='tight')  # Save plot as 'Reg SHAP 06.png'\n",
    "\n",
    "# Step 5: Calculate mean absolute SHAP values for overall feature importance\n",
    "# We compute the mean absolute SHAP values across all instances to get an importance score for each feature\n",
    "feature_imp = np.mean(np.abs(shap_values), axis=0)\n",
    "feature_imp.shape  # Check the shape to ensure compatibility with features\n",
    "\n",
    "# Step 6: Identify the top 10 most important features based on SHAP values\n",
    "ind = feature_imp.argsort()[-10:]  # Get indices of the top 10 features by importance\n",
    "ind = ind[::-1]  # Reverse indices for descending order\n",
    "\n",
    "# Display the names of the top 10 features and their importance scores\n",
    "np.array(x_df.columns)[ind]  # Names of the top 10 important features\n",
    "feature_imp[ind]  # Corresponding SHAP importance scores for the top features\n",
    "\n",
    "\n",
    "# Plot top features by SHAP absolute importance as a bar chart\n",
    "# This bar chart shows the top features contributing to model predictions in terms of SHAP importance\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size for the plot\n",
    "\n",
    "# Create a bar plot for the top features\n",
    "# - x specifies feature names (top features based on SHAP importance)\n",
    "# - y specifies the SHAP importance scores for each feature\n",
    "# - color applies a custom color setting for visual appeal\n",
    "plot = sns.barplot(x=np.array(x_df.columns)[ind], y=feature_imp[ind], color=[0.1, 0.2, 0.1])\n",
    "\n",
    "# Customize x-axis labels for clarity and appearance\n",
    "plot.set_xticklabels(plot.get_xticklabels(), horizontalalignment='center', size=12)\n",
    "\n",
    "# Set font size and weight for y-axis and x-axis ticks\n",
    "plt.yticks(size=15, weight='bold')\n",
    "plt.xticks(size=20, rotation=20, weight='bold')\n",
    "\n",
    "# Set y-axis label for the plot with custom font size and weight\n",
    "plt.ylabel('SHAP Absolute Feature Importance', size=15, weight='bold')\n",
    "\n",
    "# Save the feature importance bar plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('Reg SHAP Feature Importance CB 06', dpi=100, bbox_inches='tight')  # Save as 'Reg SHAP Feature Importance CB 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential Artificial Neural Network (ANN) model for regression\n",
    "# This model includes several Dense layers with L2 regularization to prevent overfitting\n",
    "\n",
    "# Step 1: Initialize the Sequential model\n",
    "ANN_reg = models.Sequential()\n",
    "\n",
    "# Step 2: Add Dense layers with ReLU activation and L2 regularization\n",
    "# - Each Dense layer has a specified number of neurons and L2 regularization to avoid overfitting\n",
    "# - input_dim=1024 in the first layer specifies the input feature size\n",
    "\n",
    "# First Dense layer with 100 neurons\n",
    "ANN_reg.add(layers.Dense(100, input_dim=1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Second Dense layer with 200 neurons\n",
    "ANN_reg.add(layers.Dense(200, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Third Dense layer with 300 neurons\n",
    "ANN_reg.add(layers.Dense(300, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Fourth Dense layer with 100 neurons\n",
    "ANN_reg.add(layers.Dense(100, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Fifth Dense layer with 10 neurons\n",
    "ANN_reg.add(layers.Dense(10, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Output layer with a single neuron (no activation) for regression output\n",
    "ANN_reg.add(layers.Dense(1))\n",
    "\n",
    "# Step 3: Compile the model\n",
    "# - optimizer='adam' uses the Adam optimization algorithm, which is efficient for large datasets\n",
    "# - loss='mse' specifies Mean Squared Error as the loss function, suitable for regression tasks\n",
    "# - metrics=['mse'] evaluates Mean Squared Error on each batch and epoch\n",
    "ANN_reg.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Step 4: Define EarlyStopping callback to prevent overfitting\n",
    "# - monitor='val_loss' stops training when the validation loss stops improving\n",
    "# - patience=25 specifies that training will stop if no improvement is observed for 25 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min')\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# Step 5: Train (fit) the ANN model on the training data with validation data and early stopping\n",
    "# - epochs=500 specifies the maximum number of epochs\n",
    "# - batch_size=50 defines the number of samples per gradient update\n",
    "# - validation_data=(X_val, Y_val.iloc[:, 0]) provides validation data for monitoring performance\n",
    "history2 = ANN_reg.fit(X_train, Y_train.iloc[:, 0], epochs=500, batch_size=50, validation_data=(X_val, Y_val.iloc[:, 0]), callbacks=callbacks)\n",
    "\n",
    "# Step 6: Generate predictions on training, validation, and test data\n",
    "# - These predictions will be used to evaluate model performance\n",
    "pred_train = ANN_reg.predict(X_train)  # Predictions on training data\n",
    "pred_val = ANN_reg.predict(X_val)      # Predictions on validation data\n",
    "pred_test = ANN_reg.predict(X_test)    # Predictions on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21993e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop and train a Random Forest (RF) regression model with specified hyperparameters\n",
    "# This model will be used to predict a continuous target variable and interpret feature importance using SHAP\n",
    "\n",
    "# Step 1: Define and train the Random Forest model with specified parameters\n",
    "# - n_estimators=3000 specifies the number of trees in the forest\n",
    "# - max_depth=12 limits the depth of each tree to control overfitting\n",
    "rf_default = RandomForestRegressor(n_estimators=3000, max_depth=12)\n",
    "rf_default.fit(X_train, Y_train.iloc[:, 0])  # Train the model on the training data\n",
    "\n",
    "# Step 2: Print R² score (coefficient of determination) for training, validation, and test sets\n",
    "# - rf_default.score() computes R², indicating the proportion of variance explained by the model\n",
    "print(rf_default.score(X_train, Y_train.iloc[:, 0]))  # R² score for training set\n",
    "print(rf_default.score(X_val, Y_val.iloc[:, 0]))      # R² score for validation set\n",
    "print(rf_default.score(X_test, Y_test.iloc[:, 0]))    # R² score for test set\n",
    "\n",
    "# Step 3: Generate predictions for training, validation, and test sets\n",
    "pred_train = rf_default.predict(X_train)\n",
    "pred_val = rf_default.predict(X_val)\n",
    "pred_test = rf_default.predict(X_test)\n",
    "\n",
    "# Step 4: Apply SHAP analysis to interpret feature importance for the RF model\n",
    "\n",
    "# Generate predictions on the test set and prepare data for SHAP analysis\n",
    "all_preds = rf_default.predict(X_test)\n",
    "X_df = pd.DataFrame(X_test)  # Convert X_test to a DataFrame for easier manipulation\n",
    "\n",
    "# Create deep copies of the test data for SHAP analysis and reference\n",
    "x_df = X_df.copy(deep=True)\n",
    "x_df_1st = x_df.copy(deep=True)\n",
    "x_df_1st['1st'] = all_preds  # Add predictions to the DataFrame for reference\n",
    "\n",
    "# Reset indices in x_df and x_df_1st for a clean structure\n",
    "x_df = x_df.reset_index().drop('index', axis=1)\n",
    "x_df_1st = x_df_1st.reset_index().drop('index', axis=1)\n",
    "\n",
    "# Step 5: Generate SHAP values using TreeExplainer for the Random Forest model\n",
    "# TreeExplainer calculates SHAP values, indicating each feature's contribution to predictions\n",
    "shap_values = shap.TreeExplainer(rf_default).shap_values(x_df)\n",
    "\n",
    "# Step 6: Display SHAP summary plot for feature importance\n",
    "# - This plot shows the effect of each feature on the model's output for individual predictions\n",
    "# - plot_size specifies dimensions, max_display limits the number of displayed features, and plot_type='dot' uses dots to represent values\n",
    "shap.summary_plot(shap_values, x_df, plot_size=(10, 10), show=False, plot_type='dot', max_display=10)\n",
    "plt.title('SHAP for Random Forest', weight='bold', size=20)  # Set plot title with custom font size\n",
    "plt.xticks(size=20, weight='bold')  # Customize x-axis tick labels\n",
    "plt.yticks(size=20, weight='bold')  # Customize y-axis tick labels\n",
    "plt.savefig('RF Reg SHAP 06', dpi=100, bbox_inches='tight')  # Save plot as 'RF Reg SHAP 06.png'\n",
    "\n",
    "# Step 7: Calculate mean absolute SHAP values for overall feature importance\n",
    "# We compute the mean absolute SHAP values across all instances to get an importance score for each feature\n",
    "feature_imp = np.mean(np.abs(shap_values), axis=0)\n",
    "feature_imp.shape  # Check the shape to ensure compatibility with features\n",
    "\n",
    "# Step 8: Identify the top 10 most important features based on SHAP values\n",
    "ind = feature_imp.argsort()[-10:]  # Get indices of the top 10 features by importance\n",
    "ind = ind[::-1]  # Arrange indices in descending order of importance\n",
    "\n",
    "# Display the names of the top 10 features and their SHAP importance scores\n",
    "np.array(x_df.columns)[ind]  # Names of the top 10 important features\n",
    "feature_imp[ind]  # Corresponding SHAP importance scores for the top features\n",
    "\n",
    "# Step 9: Plot top features by SHAP absolute importance as a bar chart\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size for the plot\n",
    "plot = sns.barplot(x=np.array(x_df.columns)[ind], y=feature_imp[ind], color=[0.1, 0.2, 0.1])  # Bar plot of feature importance\n",
    "\n",
    "# Customize x-axis labels for clarity and appearance\n",
    "plot.set_xticklabels(plot.get_xticklabels(), horizontalalignment='center', size=12)\n",
    "\n",
    "# Set font size and weight for y-axis and x-axis ticks\n",
    "plt.yticks(size=15, weight='bold')\n",
    "plt.xticks(size=20, rotation=20, weight='bold')\n",
    "\n",
    "# Set y-axis label for the plot with custom font size and weight\n",
    "plt.ylabel('SHAP Absolute Feature Importance', size=15, weight='bold')\n",
    "\n",
    "# Save the feature importance bar plot to a file with specified resolution and tight bounding box\n",
    "plt.savefig('RF Reg SHAP Feature Importance CB 06', dpi=100, bbox_inches='tight')  # Save as 'RF Reg SHAP Feature Importance CB 06.png'\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor (GBT) model development and tuning\n",
    "\n",
    "# Step 1: Initialize the target variable for training\n",
    "# - Select the 'Values' column from Y_train as the target variable for regression\n",
    "Y_train = Y_train['Values']\n",
    "\n",
    "# Step 2: Initialize the Gradient Boosting Regressor with a random seed for reproducibility\n",
    "Ada = GradientBoostingRegressor(random_state=42)\n",
    "Ada.fit(X_train, Y_train)  # Fit the model on the training data\n",
    "\n",
    "# Step 3: Evaluate the model on the test set by calculating the R² score\n",
    "# - Ada.score() calculates R², indicating the proportion of variance explained by the model\n",
    "Ada.score(X_test, Y_test)\n",
    "\n",
    "# Step 4: Set up a parameter grid for hyperparameter tuning\n",
    "# - max_depth controls the depth of each tree (higher values increase complexity)\n",
    "# - n_estimators specifies the number of boosting stages (trees) to build\n",
    "# - learning_rate scales the contribution of each tree, controlling the speed of learning\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 10],\n",
    "    'n_estimators': [30, 100, 1000, 3000],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Step 5: Initialize the Gradient Boosting Regressor for grid search\n",
    "ada = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Step 6: Perform Grid Search with cross-validation to find the best parameters\n",
    "# - GridSearchCV systematically tests all parameter combinations from param_grid\n",
    "# - cv=10 specifies 10-fold cross-validation\n",
    "# - n_jobs=-1 utilizes all available CPU cores for parallel processing\n",
    "# - scoring='r2' optimizes for the R² metric\n",
    "grid_search = GridSearchCV(estimator=ada, param_grid=param_grid, cv=10, n_jobs=-1, verbose=2, scoring='r2')\n",
    "\n",
    "# Step 7: Fit the grid search to the training data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Retrieve the best parameters and model from grid search\n",
    "grid_search.best_params_  # Display the best parameter combination\n",
    "best_grid_GBT = grid_search.best_estimator_  # Get the best estimator based on cross-validation\n",
    "best_grid_GBT\n",
    "\n",
    "# Step 8: Evaluate the best model on the test set using the R² score\n",
    "best_grid_GBT.score(X_test, Y_test)\n",
    "\n",
    "# Step 9: Train a new GBT model with manually selected hyperparameters\n",
    "# - n_estimators=3000 and max_depth=12 for high-capacity model\n",
    "Ada = GradientBoostingRegressor(n_estimators=3000, max_depth=12, random_state=42)\n",
    "Ada.fit(X_train, Y_train.iloc[:, 0])  # Fit the model to the training data\n",
    "\n",
    "# Step 10: Evaluate the final model on the test set\n",
    "Ada.score(X_test, Y_test.iloc[:, 0])  # Calculate the R² score on test data\n",
    "\n",
    "# Step 11: Generate predictions for training, validation, and test sets for further evaluation\n",
    "pred_train = Ada.predict(X_train)\n",
    "pred_val = Ada.predict(X_val)\n",
    "pred_test = Ada.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop and train a Support Vector Regression (SVR) model\n",
    "# SVR is a type of Support Vector Machine (SVM) tailored for regression tasks\n",
    "\n",
    "# Step 1: Initialize the SVR model with specified hyperparameters\n",
    "# - kernel='rbf' specifies the use of a radial basis function (RBF) kernel, suitable for non-linear relationships\n",
    "# - degree=3 is relevant only for polynomial kernels; here, it’s kept at the default value\n",
    "# - C=3 controls the regularization strength, with higher values potentially leading to more overfitting\n",
    "SVR = svm.SVR(kernel='rbf', degree=3, C=3)\n",
    "\n",
    "# Step 2: Train the SVR model on the training data\n",
    "# - X_train contains the features, and Y_train.iloc[:, 0] is the target variable for regression\n",
    "SVR.fit(X_train, Y_train.iloc[:, 0])\n",
    "\n",
    "# Step 3: Generate predictions on the training, validation, and test sets for model evaluation\n",
    "pred_train = SVR.predict(X_train)  # Predictions on training data\n",
    "pred_val = SVR.predict(X_val)      # Predictions on validation data\n",
    "pred_test = SVR.predict(X_test)    # Predictions on test data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
